{
 "metadata": {
  "name": "",
  "signature": "sha256:2cc2e43841f0a473178e2f279edd9258d824ab9bf1e6c91bc93f2f87ab9f51dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Educe\n",
      "\n",
      "[Educe][educe] is a library for working with a variety of discourse corpora.\n",
      "This tutorial aims to show what using educe would be like when working with\n",
      "the [STAC][stac] corpus. \n",
      "\n",
      "We'll be working with a tiny fragment of the corpus included with educe.\n",
      "You may find it useful to symlink your larger copy from\n",
      "the STAC distribution and modify this tutorial accordingly.\n",
      "\n",
      "## Installation\n",
      "\n",
      "```shell\n",
      "git clone https://github.com/kowey/educe.git\n",
      "cd educe\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "Note: these instructions assume you are running within\n",
      "a [virtual environment][virtualenv].\n",
      "If not, and if you have permission\n",
      "denied errors, replace `pip` with `sudo pip`.\n",
      "\n",
      "## Tutorial in browser (optional)\n",
      "\n",
      "This tutorial can either be followed along with the command line and your favourite\n",
      "text editor, or embedded in an interactive webpage via iPython:\n",
      "\n",
      "```shell\n",
      "pip install ipython\n",
      "cd tutorials\n",
      "ipython notebook\n",
      "```\n",
      "\n",
      "[stac]: http://www.irit.fr/STAC/\n",
      "[educe]: http://kowey.github.io/educe\n",
      "[virtualenv]: http://virtualenv.readthedocs.org/en/latest/\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reading corpus files (STAC)\n",
      "\n",
      "It helps to know here that an educe corpus is a mapping from [file id keys](https://educe.readthedocs.org/en/latest/api-doc/educe.html#educe.corpus.FileId) to Documents. The `FileId` tells us what makes a Document distinct from another:\n",
      "\n",
      "* document (eg. s1-league2-game1): in STAC, the game that was played (here, season 1, league 2, game 1)\n",
      "* subdocument (eg. 05): a mostly arbitrary subdivision of the documents motivated by technical constraints (overly large documents would cause our annotation tool to crash)\n",
      "* stage (eg. units, discourse, parsed): the kinds of annotations available in the document\n",
      "* annotator (eg. hjoseph): the main annotator for a document (gold standard documents have the distinguished annotators, BRONZE, SILVER, or GOLD)\n",
      "\n",
      "NB: unfortunately we have overloaded the word \u201cdocument\u201d here. When talking about file ids, \u201cdocument\u201d refers to a whole game.  But when talking about actual annotation objects an educe Document actually corresponds to a specific combination of document, subdocument, stage, and annotator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import educe.stac\n",
      "\n",
      "# relative to the educe docs directory\n",
      "data_dir = '../data'\n",
      "corpus_dir = '{dd}/stac-sample'.format(dd=data_dir)\n",
      "\n",
      "# read everything from our sample\n",
      "reader = educe.stac.Reader(corpus_dir)\n",
      "corpus = reader.slurp(verbose=True)\n",
      "\n",
      "# print a text fragment from the first ten files we read\n",
      "for key in corpus.keys()[:10]:\n",
      "    doc = corpus[key]\n",
      "    print(\"[{0}] {1}\".format(key, doc.text()[:50]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [0/100]\r",
        "Slurping corpus dir [1/100]\r",
        "Slurping corpus dir [2/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [3/100]\r",
        "Slurping corpus dir [4/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [5/100]\r",
        "Slurping corpus dir [6/100]\r",
        "Slurping corpus dir [7/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [8/100]\r",
        "Slurping corpus dir [9/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [10/100]\r",
        "Slurping corpus dir [11/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [12/100]\r",
        "Slurping corpus dir [13/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [14/100]\r",
        "Slurping corpus dir [15/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [16/100]\r",
        "Slurping corpus dir [17/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [18/100]\r",
        "Slurping corpus dir [19/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [20/100]\r",
        "Slurping corpus dir [21/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [22/100]\r",
        "Slurping corpus dir [23/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [24/100]\r",
        "Slurping corpus dir [25/100]\r",
        "Slurping corpus dir [26/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [27/100]\r",
        "Slurping corpus dir [28/100]\r",
        "Slurping corpus dir [29/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [30/100]\r",
        "Slurping corpus dir [31/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [32/100]\r",
        "Slurping corpus dir [33/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [34/100]\r",
        "Slurping corpus dir [35/100]\r",
        "Slurping corpus dir [36/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [37/100]\r",
        "Slurping corpus dir [38/100]\r",
        "Slurping corpus dir [39/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [40/100]\r",
        "Slurping corpus dir [41/100]\r",
        "Slurping corpus dir [42/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [43/100]\r",
        "Slurping corpus dir [44/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [45/100]\r",
        "Slurping corpus dir [46/100]\r",
        "Slurping corpus dir [47/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [48/100]\r",
        "Slurping corpus dir [49/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [50/100]\r",
        "Slurping corpus dir [51/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [52/100]\r",
        "Slurping corpus dir [53/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [54/100]\r",
        "Slurping corpus dir [55/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [56/100]\r",
        "Slurping corpus dir [57/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [58/100]\r",
        "Slurping corpus dir [59/100]\r",
        "Slurping corpus dir [60/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [61/100]\r",
        "Slurping corpus dir [62/100]\r",
        "Slurping corpus dir [63/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [64/100]\r",
        "Slurping corpus dir [65/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [66/100]\r",
        "Slurping corpus dir [67/100]\r",
        "Slurping corpus dir [68/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [69/100]\r",
        "Slurping corpus dir [70/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [71/100]\r",
        "Slurping corpus dir [72/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [73/100]\r",
        "Slurping corpus dir [74/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [75/100]\r",
        "Slurping corpus dir [76/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [77/100]\r",
        "Slurping corpus dir [78/100]\r",
        "Slurping corpus dir [79/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [80/100]\r",
        "Slurping corpus dir [81/100]\r",
        "Slurping corpus dir [82/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [83/100]\r",
        "Slurping corpus dir [84/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [85/100]\r",
        "Slurping corpus dir [86/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [87/100]\r",
        "Slurping corpus dir [88/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [89/100]\r",
        "Slurping corpus dir [90/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [91/100]\r",
        "Slurping corpus dir [92/100]\r",
        "Slurping corpus dir [93/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [94/100]\r",
        "Slurping corpus dir [95/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [96/100]\r",
        "Slurping corpus dir [97/100]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [98/100]\r",
        "Slurping corpus dir [99/100]\r",
        "Slurping corpus dir [100/100 done]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[s1-league2-game1 [05] unannotated None]  199 : sabercat : anyone any clay? 200 : IG : nope\n",
        "[s1-league2-game1 [13] units hjoseph]  521 : sabercat : skinnylinny 522 : sabercat : som\n",
        "[s1-league2-game1 [10] units hjoseph]  393 : skinnylinny : Shall we extend? 394 : saberc\n",
        "[s1-league2-game1 [11] discourse hjoseph]  450 : skinnylinny : Argh 451 : skinnylinny : How \n",
        "[s1-league2-game1 [05] units SILVER]  199 : sabercat : anyone any clay? 200 : IG : nope\n",
        "[s1-league2-game1 [02] units lpetersen]  75 : sabercat : anyone has any wood? 76 : skinnyl\n",
        "[s1-league2-game3 [05] units lpetersen]  283 : skinnylinny : Oh, sabercat, want to trade a\n",
        "[s1-league2-game3 [03] discourse lpetersen]  151 : amycharl : got wood anyone? 152 : sabercat \n",
        "[s1-league2-game1 [10] discourse hjoseph]  393 : skinnylinny : Shall we extend? 394 : saberc\n",
        "[s1-league2-game1 [12] units SILVER]  496 : sabercat : yes! 497 : sabercat : :D 498 : s\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Faster reading\n",
      "\n",
      "If you know that you only want to work with a subset of the corpus files, you can pre-filter the corpus before reading the files. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "# nb: you can import this function from educe.stac.corpus\n",
      "def is_metal(fileid):  \n",
      "    \"is this a gold standard(ish) annotation file?\"\n",
      "    anno = fileid.annotator or \"\"\n",
      "    return anno.lower() in [\"bronze\", \"silver\", \"gold\"]\n",
      "    \n",
      "# pick out gold-standard documents\n",
      "subset = reader.filter(reader.files(), \n",
      "                       lambda k: is_metal(k) and int(k.subdoc) < 4)\n",
      "corpus_subset = reader.slurp(subset, verbose=True)\n",
      "for key in corpus_subset:\n",
      "    doc = corpus_subset[key]\n",
      "    print(\"{0}: {1}\".format(key, doc.text()[:50]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [0/12]\r",
        "Slurping corpus dir [1/12]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [2/12]\r",
        "Slurping corpus dir [3/12]\r",
        "Slurping corpus dir [4/12]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [5/12]\r",
        "Slurping corpus dir [6/12]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [7/12]\r",
        "Slurping corpus dir [8/12]\r",
        "Slurping corpus dir [9/12]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [10/12]\r",
        "Slurping corpus dir [11/12]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "s1-league2-game1 [01] units SILVER:  1 : sabercat : btw, are we playing without the ot\n",
        "s1-league2-game1 [01] discourse SILVER:  1 : sabercat : btw, are we playing without the ot\n",
        "s1-league2-game1 [02] discourse SILVER:  75 : sabercat : anyone has any wood? 76 : skinnyl\n",
        "s1-league2-game3 [01] discourse BRONZE:  1 : amycharl : i made it! 2 : amycharl : did the \n",
        "s1-league2-game3 [02] discourse BRONZE:  73 : sabercat : skinny, got some ore? 74 : skinny\n",
        "s1-league2-game3 [02] units BRONZE:  73 : sabercat : skinny, got some ore? 74 : skinny\n",
        "s1-league2-game3 [01] units BRONZE:  1 : amycharl : i made it! 2 : amycharl : did the \n",
        "s1-league2-game1 [02] units SILVER:  75 : sabercat : anyone has any wood? 76 : skinnyl\n",
        "s1-league2-game1 [03] discourse SILVER:  109 : sabercat : well done! 110 : IG : More clay!\n",
        "s1-league2-game1 [03] units SILVER:  109 : sabercat : well done! 110 : IG : More clay!\n",
        "s1-league2-game3 [03] discourse BRONZE:  151 : amycharl : got wood anyone? 152 : sabercat \n",
        "s1-league2-game3 [03] units BRONZE:  151 : amycharl : got wood anyone? 152 : sabercat \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "Slurping corpus dir [12/12 done]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------\n",
      "\n",
      "# HAZARD AREA\n",
      "\n",
      "Everything below this point should be considered to be in a scratch/broken state.\n",
      "It needs to ported over from its RST/DT considerations to STAC\n",
      "\n",
      "To do:\n",
      "\n",
      "* standing off (ac/aa) - shared aa\n",
      "* layers (units/discourse)\n",
      "* grabbing resources etc (example of working with unit level annotation)\n",
      "* synchronising layers (grabbing the dialogue act and relations at the same time)\n",
      "* external annotations (postags, parse trees)\n",
      "* working with hypergraphs (implementing `_repr_png()_` would be pretty sweet)\n",
      "\n",
      "\n",
      "\n",
      "--------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Trees and annotations\n",
      "\n",
      "RST DT documents are basically trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Standing off\n",
      "\n",
      "Most annotations in the STAC corpus are [educe standoff annotations][standoff]. In educe terms, this means that they (perhaps indirectly) extend the `educe.annotation.Standoff` class and provide a `text_span()` function.  Much of our reasoning around annotations essentially consists of checking that their text spans overlap or enclose each other.\n",
      "\n",
      "As for the text spans, these refer to the raw text saved in files with an `.ac` extension (eg. `s1-league1-game3.ac`). In the [Glozz annotation tool][glozz], these `.ac` text files form a pair with their `.aa` xml counterparts.  Because we have several layers of annotations (and sometimes multiple annotators), TODO\n",
      "\n",
      "In STAC, we tend to have several Documents that refer to the same text, for example, annotations done by two different people.  \n",
      "\n",
      "Along with their `.aa` counterparts (the actual annotations)\n",
      "\n",
      "These annotation files form a pair along with their `.aa` counterparts.\n",
      "\n",
      "All Glozz documents\n",
      "\n",
      "\n",
      "[glozz]: \n",
      "[standoff]: http://educe.readthedocs.org/en/latest/api-doc/educe.html#educe.annotation.Standoff"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ex_rst_txt_filename = '{corpus}/{doc}'.format(corpus=rst_corpus_dir,\n",
      "                                              doc=ex_key.doc)\n",
      "\n",
      "with open(ex_rst_txt_filename) as ifile:\n",
      "    ex_txt = ifile.read()\n",
      "    ex_snippet_start = ex_txt.find(\"At a national\")\n",
      "    print(ex_txt[ex_snippet_start:ex_snippet_start + 500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At a nationally televised legislative session in Budapest, the Parliament overwhelmingly approved changes formally ending one-party domination in the country, regulating free elections by next summer and establishing the office of state president to replace a 21-member council.\n",
        "The country was renamed the Republic of Hungary.\n",
        "Like other Soviet bloc nations, it had been known as a \"people's republic\" since \n",
        "\n",
        "The voting for new laws followed dissolution of Hungary's Communist Party this month and \n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's have a closer look at the annotations themselves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# it may be useful to have a couple of helper functions to\n",
      "# display standoff annotations in a generic way\n",
      "def text_snippet(text):\n",
      "    \"short text fragment\"\n",
      "    if len(text) < 43:\n",
      "        return text\n",
      "    else:\n",
      "        return \"{0}...{1}\".format(text[:20], text[-20:])\n",
      "\n",
      "def preview_standoff(tystr, context, anno):\n",
      "    \"simple glimpse at a standoff annotation\"\n",
      "    span = anno.text_span()\n",
      "    text = context.text(span)\n",
      "    return \"{tystr} at {span}:\\t{snippet}\".format(tystr=tystr,\n",
      "                                                 span=span,\n",
      "                                                 snippet=text_snippet(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### EDUs and subtrees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# in educe RST/DT all annotations have a shared context object\n",
      "# that refers to an RST document; you don't always need to use\n",
      "# it, but it can be handy for writing general code like the\n",
      "# above\n",
      "ex_context = ex_doc.label().context\n",
      "\n",
      "# display some edus\n",
      "print(\"Some edus\")\n",
      "edus = ex_subtree.leaves()\n",
      "for edu in edus:\n",
      "    print(preview_standoff(\"EDU\", ex_context, edu))\n",
      "    \n",
      "print(\"\\nSome subtrees\")\n",
      "# display some RST subtrees and the edus they enclose\n",
      "for subtree in ex_subtree.subtrees():\n",
      "    node = subtree.label()\n",
      "    stat = \"N\" if node.is_nucleus() else \"S\"\n",
      "    label = \"{stat} {rel: <30}\".format(stat=stat,\n",
      "                                  rel=node.rel)\n",
      "    print(preview_standoff(label, ex_context, subtree))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Some edus\n",
        "EDU at (1504,1609):\tAt a nationally tele...gly approved changes\n",
        "EDU at (1610,1662):\tformally ending one-...tion in the country,\n",
        "EDU at (1663,1703):\tregulating free elections by next summer\n",
        "EDU at (1704,1750):\tand establishing the...e of state president\n",
        "EDU at (1751,1782):\tto replace a 21-member council.\n",
        "\n",
        "Some subtrees\n",
        "S elaboration-general-specific   at (1504,1782):\tAt a nationally tele...a 21-member council.\n",
        "N span                           at (1504,1609):\tAt a nationally tele...gly approved changes\n",
        "S elaboration-object-attribute-e at (1610,1782):\tformally ending one-...a 21-member council.\n",
        "N List                           at (1610,1662):\tformally ending one-...tion in the country,\n",
        "N List                           at (1663,1703):\tregulating free elections by next summer\n",
        "N List                           at (1704,1782):\tand establishing the...a 21-member council.\n",
        "N span                           at (1704,1750):\tand establishing the...e of state president\n",
        "S purpose                        at (1751,1782):\tto replace a 21-member council.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Penn Treebank integration\n",
      "\n",
      "RST DT annotations are mostly over Wall Street Journal articles from the Penn Treebank. If you have a copy of the latter at the ready, you can ask educe to read and align the two (ie. PTB annotations treated as standing off the RST source text).  This alignment consists of some universal substitutions (eg. `-LBR-` to `(`) and with a [bit of hardcoding][ugh] to account for seemingly random differences in whitespace/punctuation.\n",
      "\n",
      "[ugh]: https://github.com/kowey/educe/blob/master/educe/rst_dt/ptb.py\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combining annotations\n",
      "\n",
      "We now have several types of annotation at our disposal:\n",
      "\n",
      "* EDUs and RST trees\n",
      "* raw text paragraph/sentences (not terribly reliable)\n",
      "* PTB trees\n",
      "\n",
      "The next question that arises is how we can use these annotations in conjuction with each other.\n",
      "\n",
      "### Span enclosure and overlapping\n",
      "\n",
      "The simplest way to reason about annotations (particularly since they tend to be sloppy and to overlap).  Suppose for example, we wanted to find all of the edus in a tree that are in the same sentence as an given edu. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "\n",
      "# pick an EDU, any edu\n",
      "ex_edus = ex_subtree.leaves()\n",
      "ex_edu0 = ex_edus[3]\n",
      "print(preview_standoff('example EDU', ex_context, ex_edu0))\n",
      "\n",
      "# all of the sentences in the example document\n",
      "ex_sents = list(chain.from_iterable(x.sentences for x in ex_context.paragraphs))\n",
      "\n",
      "# sentences that overlap the edu\n",
      "# (we use overlaps instead of encloses because edus might\n",
      "# span sentence boundaries)\n",
      "ex_edu0_sents = [x for x in ex_sents if x.overlaps(ex_edu0)]\n",
      "\n",
      "# and now the edus that overlap those sentences\n",
      "ex_edu0_buddies = []\n",
      "for sent in ex_edu0_sents:\n",
      "    print(preview_standoff('overlapping sentence', ex_context, sent))\n",
      "    buddies = [x for x in ex_edus if x.overlaps(sent)]\n",
      "    buddies.remove(ex_edu0)\n",
      "    for edu in buddies:\n",
      "        print(preview_standoff('\\tnearby EDU', ex_context, edu))\n",
      "    ex_edu0_buddies.extend(buddies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "example EDU at (1704,1750):\tand establishing the...e of state president\n",
        "overlapping sentence at (1504,1782):\tAt a nationally tele...a 21-member council.\n",
        "\tnearby EDU at (1504,1609):\tAt a nationally tele...gly approved changes\n",
        "\tnearby EDU at (1610,1662):\tformally ending one-...tion in the country,\n",
        "\tnearby EDU at (1663,1703):\tregulating free elections by next summer\n",
        "\tnearby EDU at (1751,1782):\tto replace a 21-member council.\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Span example 2 (exercise)\n",
      "\n",
      "As an exercise, how about extracting the PTB part of speech tags for every token in our example EDU?  How for example, would you determine if an EDU contains a VBG-tagged word?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ex_postags = list(chain.from_iterable(t.leaves() for t in ptb_trees[ex_key]))\n",
      "\n",
      "print(\"some of the POS tags\")\n",
      "for postag in ex_postags[300:310]:\n",
      "    print(preview_standoff(postag.tag, ex_context, postag))\n",
      "    \n",
      "print()\n",
      "ex_edu0_postags = [] # EXERCISE <-- fill this in\n",
      "print(\"has VBG? \", ) # EXERCISE <-- fill this in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "some of the POS tags\n",
        "VBG at (1663,1673):\tregulating\n",
        "JJ at (1674,1678):\tfree\n",
        "NNS at (1679,1688):\telections\n",
        "IN at (1689,1691):\tby\n",
        "JJ at (1692,1696):\tnext\n",
        "NN at (1697,1703):\tsummer\n",
        "CC at (1704,1707):\tand\n",
        "VBG at (1708,1720):\testablishing\n",
        "DT at (1721,1724):\tthe\n",
        "NN at (1725,1731):\toffice\n",
        "\n",
        "has VBG? \n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tree searching\n",
      "\n",
      "The same span enclosure logic can be used to search parse trees for particular constituents, verb phrases. Alternatively, you can use the the `topdown` method provided by educe trees. This returns just the largest constituent for which some predicate is true.  It optionally accepts an additional argument to cut off the search when it is clearly out of bounds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ex_ptb_trees = ptb_trees[ex_key]\n",
      "ex_edu0_ptb_trees = [x for x in ex_ptb_trees if x.overlaps(ex_edu0)]\n",
      "ex_edu0_cons = []\n",
      "for ptree in ex_edu0_ptb_trees:\n",
      "    print(preview_standoff('ptb tree', ex_context, ptree))\n",
      "    ex_edu0_cons.extend(ptree.topdown(lambda c: ex_edu0.encloses(c)))\n",
      "    \n",
      "# the largest constituents enclosed by this edu\n",
      "for cons in ex_edu0_cons:\n",
      "    print(preview_standoff(cons.label(), ex_context, cons))\n",
      "    \n",
      "display(ex_edu0_cons[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ptb tree at (1504,1782):\tAt a nationally tele...a 21-member council.\n",
        "CC at (1704,1707):\tand\n",
        "VBG at (1708,1720):\testablishing\n",
        "NP at (1721,1731):\tthe office\n",
        "PP at (1732,1750):\tof state president\n",
        "WHNP-1 at (1750,1750):\t\n",
        "NP-SBJ at (1750,1750):\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "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",
       "text": [
        "ConstituencyTree('PP', [ConstituencyTree('IN', [<educe.external.postag.Token object at 0x10c3e7110>]), ConstituencyTree('NP', [ConstituencyTree('NN', [<educe.external.postag.Token object at 0x10c3e71d0>]), ConstituencyTree('NN', [<educe.external.postag.Token object at 0x10c3e7290>])])])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusion\n",
      "\n",
      "In this tutorial, we've explored a couple of basic educe concepts, which we hope will enable you to extract some data from your discourse corpora, namely\n",
      "\n",
      "* reading corpus data (and pre-filtering)\n",
      "* standoff annotations\n",
      "* searching by span enclosure, overlapping\n",
      "* working with trees\n",
      "* combining annotations from different sources\n",
      "\n",
      "The concepts above should transfer to whatever discourse corpus you are working with (that educe supports, or that you are prepared to supply a reader for).\n",
      "\n",
      "### Work in progress\n",
      "\n",
      "This tutorial is very much a work in progress (last update: 2014-09-19).\n",
      "Educe is a bit of a moving target, so [let me know](https://github.com/kowey/educe/issues) if you run into any trouble!\n",
      "\n",
      "### See also\n",
      "\n",
      "#### stac-util\n",
      "\n",
      "Some of the things you may want to do with the STAC corpus may already exist in the stac-util command line tool.\n",
      "stac-util is meant to be a sort of Swiss Army Knife, providing tools for editing the corpus. The query tools are more likely to be of interest:\n",
      "\n",
      "* text: display text and edu/dialogue segmentation in a friendly way\n",
      "* graph: draw discourse graphs with graphviz (arrows for relations, boxes for CDUs, etc)\n",
      "* filter-graph: visualise instances of relations (eg. Question answer pair)\n",
      "* count: generate statistics about the corpus\n",
      "\n",
      "See `stac-util --help` for more details.\n",
      "\n",
      "#### External tool support\n",
      "\n",
      "Educe has some support for reading data from outside the discourse corpus proper.  For example, if you run the stanford corenlp parser on the raw text, you can read them back into educe-style `ConstituencyTree` and `DependencyTree` annotations. See [educe.external](https://educe.readthedocs.org/en/latest/api-doc/educe.external.html) for details.\n",
      "\n",
      "If you have a part of speech tagger that you would like to use, the `educe.external.postag` module may be useful for representing the annotations that come out of it\n",
      "\n",
      "You can also add support for your own tools by creating annotations that extend `Standoff`, directly or otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}